{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Maximum-likelihood fitting\n",
    "\n",
    "In this exercise we will implement a maximum-likelihood fit using the common example of fitting a Gaussian curve to some normally-distributed data.\n",
    "\n",
    "The main task will be to implement a gradient-descent algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "The objective of fitting a PDF to data is to find the values of the PDF's parameters that maximise the likelihood, thus giving the best description of a dataset (i.e. the best fit).\n",
    "\n",
    "While it's possible to store parameters as simple `float`s, we ask that you implement a class that stores the parameter value and a range that the value cannot stray outside during the fit. This will be especially helpful while testing and debugging your minimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDFs and likelihoods\n",
    "\n",
    "A Gaussian PDF is defined as:\n",
    "$$\n",
    "f(x|\\mu,\\sigma) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "The likelihood is the product of the PDF evaluated on all points $\\{x_i\\}$ in a dataset (assuming all points are independent):\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mu,\\sigma) = \\prod_i f(x_i|\\mu,\\sigma)\n",
    "$$\n",
    "\n",
    "Rather than maximise the likelihood, it is easier computationally to minimise the negative log-likelihood:\n",
    "$$\n",
    "-\\log(\\mathcal{L}(\\mu,\\sigma)) = -\\sum_i\\log(f(x_i|\\mu,\\sigma))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimisation via gradient descent\n",
    "\n",
    "Gradient descent is a method of finding the local minimum of a differentiable function by following the local gradient.\n",
    "\n",
    "Note that the local minimum may not be the same as the global minimum. There are strategies to avoid a local-but-not-global minimum, such as performing a fit multiple times with random starting values.\n",
    "\n",
    "Say we have a multivariate function $L(\\vec{\\theta})$, where $\\vec{\\theta} = [\\theta_1, \\theta_2, ..., \\theta_n].$ _**NB**: in this exercise, $L(\\vec{\\theta})=-\\log(\\mathcal{L}(\\mu,\\sigma))$, the negative log-likelihood._\n",
    "\n",
    "The gradient of the function is $$\\nabla_\\theta L(\\vec{\\theta}) = \\left[\\frac{\\partial L}{\\partial \\theta_1}, \\frac{\\partial L}{\\partial \\theta_2}, ..., \\frac{\\partial L}{\\partial \\theta_n}\\right].$$\n",
    "\n",
    "Starting from some initial position $\\vec{\\theta}_i$, one can descend the gradient towards the minimum by subtracting an amount proportional to the local gradient: $$\\vec{\\theta}_{t} = \\vec{\\theta}_{t-1} - \\eta_t \\nabla_\\theta L(\\vec{\\theta}_{t-1}),$$ where $\\eta_t$ is the \"step size\". This can be repeated until $\\vec{\\theta}$ converges on the values that minimise $L(\\vec{\\theta})$ (i.e. the best-fit values $\\vec{\\hat{\\theta}}$).\n",
    "\n",
    "The exact criterion for achieving convergence is up to you to decide and implement.\n",
    "A good starting point is to use the relative change $$\\left|\\frac{L(\\vec{\\theta}_{i})-L(\\vec{\\theta}_{i-1})}{L(\\vec{\\theta}_{i})+L(\\vec{\\theta}_{i-1})}\\right|$$ and stop when it goes below some threshold.\n",
    "\n",
    "Choosing an appropriate step size $\\eta_t$ is crucial for an optimal balance between speed and precision, and many methods are available for doing this. Note that it does not need to be a fixed size and can be adjusted at each iteration.\n",
    "\n",
    "### Batch vs stochastic\n",
    "\n",
    "In batch gradient descent, the parameters are updated using the likelihood calculated over the full dataset.\n",
    "\n",
    "In stochastic gradient descent, the parameters are updated for each datapoint (using $\\nabla_\\theta L(\\vec{\\theta}, x_i)$), or a sub-sample of the full dataset (mini-batch).\n",
    "\n",
    "*See the lecture notes for more.*\n",
    "\n",
    "### Momentum\n",
    "\n",
    "Useful particularly in stochastic/mini-batch gradient descent is the idea of 'momentum'. Where the parameters are updated using: $$\\vec{\\theta}_t = \\vec{\\theta}_{t-1} - \\vec{v}_t,$$ where $$\\vec{v}_t = \\gamma \\vec{v}_{t-1} + \\eta_t \\nabla_\\theta L(\\vec{\\theta}_{t-1}, x),$$ where $\\gamma$ is the momentum parameter $0 < \\gamma < 1$ (typically around 0.9).\n",
    "\n",
    "### Nesterov's accelerated gradient descent\n",
    "\n",
    "Using momentum we can 'look ahead' to where the next update will be approximately, without calculating a new gradient: $$\\vec{\\theta}_t \\approx \\vec{\\theta}_{t-1} - \\gamma \\vec{v}_{t-1}$$\n",
    "\n",
    "We can instead use that position when calculating the new gradient, so $\\vec{v}_t$ becomes: $$\\vec{v}_t = \\gamma \\vec{v}_{t-1} + \\eta_t \\nabla_\\theta L(\\vec{\\theta}_{t-1} - \\gamma \\vec{v}_{t-1}, x).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing your own gradient-descent minimiser\n",
    "\n",
    "Here you should implement:\n",
    "1. A parameter class that holds the value and allowed range of a parameter\n",
    "  - When setting a value outside the allowed range, force the value to equal the nearest boundary\n",
    "2. A Gaussian PDF class or function using parameters that control its mean and standard deviation\n",
    "3. A gradient-descent minimiser which iteratively:\n",
    "  - Calcualtes the likelihood gradient at the current values of the fit parameters\n",
    "  - Updates the parameters following the gradient\n",
    "  - Saves the likelihood in a list (for plotting later)\n",
    "  - Stops the fit if it has converged (or if a maximum number of iterations have been reached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "56.998500000077\n",
      "-1083.6281717150769\n"
     ]
    }
   ],
   "source": [
    "# Implement the parameter class, the Guassian PDF and gradient-descent minimisation here\n",
    "# You can create extra cells if you wish\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import copy\n",
    "\n",
    "class Theta:\n",
    "    value = None\n",
    "    upperBound = None\n",
    "    lowerBound = None\n",
    "\n",
    "    def __init__(self, lowerBound, upperBound):\n",
    "        if upperBound < lowerBound:\n",
    "            print(\"upper boundary lower than lower boundary\")\n",
    "        elif upperBound == lowerBound:\n",
    "            print(\"upper boundary equal to lower boundary\")\n",
    "        else:\n",
    "            self.upperBound = upperBound\n",
    "            self.lowerBound = lowerBound\n",
    "\n",
    "    def update(self, newValue):\n",
    "        self.value = np.clip(newValue, self.lowerBound, self.upperBound)\n",
    "\n",
    "param = Theta(-1,1)\n",
    "param.update(0.5)\n",
    "print(param.value)\n",
    "\n",
    "sigma = Theta(-1,1)\n",
    "mean = Theta(-20,20)\n",
    "sigma.update(1)\n",
    "mean.update(0)\n",
    "\n",
    "def normalPDF(x, params): #params is [mean,sigma]\n",
    "    # print(norm(params[0].value,params[1].value).pdf(x), x, params[0].value, params[1].value)\n",
    "    return norm(params[0].value,params[1].value).pdf(x)\n",
    "\n",
    "def logLikely(data, params, function):\n",
    "    sum = 0\n",
    "    for x in data:\n",
    "        #print(sum, \"sum\", params[0].value)\n",
    "        sum += np.log(function(x,params))\n",
    "    return -sum\n",
    "\n",
    "#likelyhood function with (dataarray, parameterarray, function)\n",
    "#theta is deepcopy\n",
    "def gradient(theta, likelyhood, data, directionIndex, eps, function):\n",
    "    theta[directionIndex].update(theta[directionIndex].value + eps/2)\n",
    "    pos = likelyhood(data,theta,function)\n",
    "    # print(\"grad\", pos, directionIndex, theta[0].value)\n",
    "    theta[directionIndex].update(theta[directionIndex].value - eps)\n",
    "    neg = likelyhood(data,theta,function)\n",
    "    # print(\"grad\", pos, neg, directionIndex, theta[0].value)\n",
    "    grad = (pos - neg)/eps\n",
    "    return grad\n",
    "\n",
    "\n",
    "def gradientStep(theta, learningRate, likelyhood, data, eps, function):\n",
    "    tmp = copy.deepcopy(theta)\n",
    "    for i,par in enumerate(theta):\n",
    "        par.update(par.value - learningRate*gradient(copy.deepcopy(tmp),likelyhood,data,i,eps,function))\n",
    "    return theta\n",
    "\n",
    "def momentumGradientStep(theta, learningRate, likelyhood, data, eps, function, gamma, v):\n",
    "    tmp = copy.deepcopy(theta)\n",
    "    for i,vi in enumerate(v):\n",
    "        # print(learningRate*gradient(copy.deepcopy(tmp),likelyhood,data,i,eps,function), i)\n",
    "        vi.update(gamma*vi.value + learningRate*gradient(copy.deepcopy(tmp),likelyhood,data,i,eps,function))\n",
    "    for i,par in enumerate(theta):\n",
    "        par.update(par.value - v[i].value)\n",
    "        print(par.value, v[i].value)\n",
    "    return v\n",
    "\n",
    "def nesterovStep(theta, learningRate, likelyhood, data, eps, function, gamma, v):\n",
    "    tmp = copy.deepcopy(theta)\n",
    "    if(v[0].value != tmp[0].value):\n",
    "        for i,par in enumerate(tmp):\n",
    "            par.update(par.value - gamma * v[i].value)\n",
    "    for i,vi in enumerate(v):\n",
    "        print(learningRate*gradient(copy.deepcopy(tmp),likelyhood,data,i,eps,function), i)\n",
    "        vi.update(gamma*vi.value + learningRate*gradient(copy.deepcopy(tmp),likelyhood,data,i,eps,function))\n",
    "    for i,par in enumerate(theta):\n",
    "        par.update(par.value - v[i].value)\n",
    "    return v\n",
    "\n",
    "def gradientDecend(theta, learningRate, likelyhood, data, Gradeps, function, maxStep, stopEps):\n",
    "    LtMinusOne = 0\n",
    "    Lt = 0\n",
    "    Step = 0 \n",
    "    while Step < maxStep:\n",
    "        LtMinusOne = likelyhood(data, theta, function)\n",
    "        gradientStep(theta, learningRate, likelyhood, data, Gradeps, function)\n",
    "        Lt = likelyhood(data, theta, function)\n",
    "        Step += 1\n",
    "        if (np.abs((LtMinusOne - Lt)))/(np.abs((LtMinusOne + Lt))) < stopEps:\n",
    "            print(Step)\n",
    "            return theta\n",
    "    return theta\n",
    "\n",
    "def stochasticGradientDecent(theta, learningRate, likelyhood, data, Gradeps, function, maxStep, stopEps,batchSize):\n",
    "    LtMinusOne = 0\n",
    "    Lt = 0\n",
    "    Step = 0 \n",
    "    while Step < maxStep:\n",
    "        LtMinusOne = likelyhood(data, theta, function)\n",
    "        gradientStep(theta, learningRate, likelyhood, np.random.choice(data,size=batchSize), Gradeps, function)\n",
    "        Lt = likelyhood(data, theta, function)\n",
    "        Step += 1\n",
    "        if (np.abs((LtMinusOne - Lt)))/(np.abs((LtMinusOne + Lt))) < stopEps:\n",
    "            print(Step)\n",
    "            return theta\n",
    "    return theta\n",
    "\n",
    "def momentumGradientDecent(theta, learningRate, likelyhood, data, Gradeps, function, maxStep, stopEps, batchSize, gamma):\n",
    "    LtMinusOne = 0\n",
    "    Lt = 0\n",
    "    Step = 0 \n",
    "    v = np.asarray([Theta(-par.upperBound,par.upperBound) for par in theta])\n",
    "    for par in v:\n",
    "        par.update(0)\n",
    "    while Step < maxStep:\n",
    "        LtMinusOne = likelyhood(data, theta, function)\n",
    "        v = momentumGradientStep(theta, learningRate, likelyhood, np.random.choice(data,size=batchSize), Gradeps, function, gamma, v)\n",
    "        Lt = likelyhood(data, theta, function)\n",
    "        Step += 1\n",
    "        if (np.abs((LtMinusOne - Lt)))/(np.abs((LtMinusOne + Lt))) < stopEps:\n",
    "            print(Step)\n",
    "            return theta\n",
    "    print(\"max steps\")\n",
    "    return theta\n",
    "\n",
    "def nesterovGradientDecent(theta, learningRate, likelyhood, data, Gradeps, function, maxStep, stopEps, batchSize, gamma):\n",
    "    LtMinusOne = 0\n",
    "    Lt = 0\n",
    "    Step = 0 \n",
    "    v = np.asarray([Theta(-par.upperBound,par.upperBound) for par in theta])\n",
    "    for par in v:\n",
    "        par.update(0)\n",
    "    while Step < maxStep:\n",
    "        LtMinusOne = likelyhood(data, theta, function)\n",
    "        v = nesterovStep(theta, learningRate, likelyhood, np.random.choice(data,size=batchSize), Gradeps, function, gamma, v)\n",
    "        Lt = likelyhood(data, theta, function)\n",
    "        Step += 1\n",
    "        if (np.abs((LtMinusOne - Lt)))/(np.abs((LtMinusOne + Lt))) < stopEps:\n",
    "            print(Step)\n",
    "            return theta\n",
    "    print(\"max steps\")\n",
    "    return theta\n",
    "\n",
    "sigma = Theta(-1,1)\n",
    "mean = Theta(-20,20)\n",
    "sigma.update(1)\n",
    "mean.update(20)\n",
    "theta = np.asarray([mean,sigma])\n",
    "data = np.asarray([0,1,2])\n",
    "\n",
    "print(gradient(copy.deepcopy(theta),logLikely,data,0,0.001,normalPDF))\n",
    "print(gradient(copy.deepcopy(theta),logLikely,data,1,0.001,normalPDF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting data\n",
    "\n",
    "The data sample provided with this sheet consists of Gaussian-distributed measurements of the $B^0$ meson mass, in units of MeV. The standard deviation of the distribution is dominated by the resolution of the detector used to make the measurement.\n",
    "\n",
    "Your task is to obtain best-fit values for the mass of the meson (i.e. the mean, $\\mu$) and the detector resolution (i.e. the standard deviation $\\sigma$) using your minimiser and Gaussian PDFs implemented above.\n",
    "\n",
    "In the cell below, the data is loaded into a [Pandas DataFrame](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) and plotted as a [Matplotlib histogram](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html). This should give you an idea of which starting values and ranges to set for the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"test_data.csv\", names=[\"mass\"])\n",
    "\n",
    "num_bins = 50\n",
    "_, bins, _ = plt.hist(df[\"mass\"], num_bins)\n",
    "bin_width = (bins[-1] - bins[0])/num_bins\n",
    "plt.xlabel(\"Mass [MeV]\")\n",
    "print(theta[0].value, theta[1].value)\n",
    "plt.ylabel(f\"Events / ({bin_width:.1f} MeV)\")\n",
    "plt.show()\n",
    "\n",
    "print(len(df[\"mass\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "5293.310504483998 102.2272710022662\n"
     ]
    }
   ],
   "source": [
    "# Use this cell to perform a fit to the data\n",
    "sigma = Theta(0,300)\n",
    "mean = Theta(5000,5600)\n",
    "sigma.update(400)\n",
    "mean.update(5250)\n",
    "theta = np.asarray([mean,sigma])\n",
    "data = np.asarray([0,1,2])\n",
    "gradientDecend(theta, 5, logLikely, df[\"mass\"], 0.001, normalPDF, 20, 0.002)\n",
    "print(theta[0].value, theta[1].value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5273.601790230714 184.74678176089583\n"
     ]
    }
   ],
   "source": [
    "sigma = Theta(0,300)\n",
    "mean = Theta(5000,5600)\n",
    "sigma.update(400)\n",
    "mean.update(5250)\n",
    "theta = np.asarray([mean,sigma])\n",
    "data = np.asarray([0,1,2])\n",
    "stochasticGradientDecent(theta, 5, logLikely, df[\"mass\"], 0.001, normalPDF, 20, 0.0001, 350)\n",
    "print(theta[0].value, theta[1].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.35318480456680845 0\n",
      "0.5942355865329318 1\n",
      "5250.353184804567 -0.35318480456680845\n",
      "149.40576441346707 0.5942355865329318\n",
      "-0.7754768121515099 0\n",
      "0.24345473534026496 1\n",
      "5251.446527940829 -1.0933431362616375\n",
      "148.62749765024716 0.7782667632199036\n",
      "-0.1607834023786836 0\n",
      "0.5794883384169225 1\n",
      "5252.591320165843 -1.1447922250141573\n",
      "147.34756922493233 1.2799284253148357\n",
      "-0.16903746356433658 0\n",
      "0.6852531770107362 1\n",
      "5253.79067063192 -1.1993504660770782\n",
      "145.51038046513824 1.8371887597940884\n",
      "-0.6528426837348889 0\n",
      "0.48159837277239603 1\n",
      "5255.522928735124 -1.7322581032042592\n",
      "143.37531220855118 2.1350682565870756\n",
      "-0.82300067603569 0\n",
      "0.6855124895821518 1\n",
      "5257.904961704044 -2.3820329689195234\n",
      "140.76823828804066 2.60707392051052\n",
      "-0.3895353970051474 0\n",
      "0.29745139045189717 1\n",
      "5260.438326773076 -2.5333650690327185\n",
      "138.1244203691293 2.643817918911365\n",
      "-0.9845348428427769 0\n",
      "0.2504454525364963 1\n",
      "5263.702890178049 -3.2645634049722236\n",
      "135.49453878957257 2.629881579556725\n",
      "0.1998512037459932 0\n",
      "0.7832959752818169 1\n",
      "5266.441146038777 -2.738255860729008\n",
      "132.3443493926897 3.1501893968828694\n",
      "-0.026792603620151567 0\n",
      "1.2271860286716674 1\n",
      "5268.932368917053 -2.491222878276259\n",
      "128.28199290682346 4.06235648586625\n",
      "0.4002253362500596 0\n",
      "0.6936997077389151 1\n",
      "5270.774244171252 -1.8418752541985732\n",
      "123.93217236180492 4.34982054501854\n",
      "-0.45892179798556754 0\n",
      "-0.42667229891435454 1\n",
      "5272.890853698016 -2.1166095267642833\n",
      "120.44400617020258 3.4881661916023314\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7683/3856467037.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5250\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmomentumGradientDecent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogLikely\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mass\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalPDF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mass\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7683/1644375810.py\u001b[0m in \u001b[0;36mmomentumGradientDecent\u001b[0;34m(theta, learningRate, likelyhood, data, Gradeps, function, maxStep, stopEps, batchSize, gamma)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mpar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mStep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmaxStep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mLtMinusOne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlikelyhood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmomentumGradientStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearningRate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlikelyhood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGradeps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mLt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlikelyhood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7683/1644375810.py\u001b[0m in \u001b[0;36mlogLikely\u001b[0;34m(data, params, function)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m#print(sum, \"sum\", params[0].value)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0msum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7683/1644375810.py\u001b[0m in \u001b[0;36mnormalPDF\u001b[0;34m(x, params)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnormalPDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#params is [mean,sigma]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# print(norm(params[0].value,params[1].value).pdf(x), x, params[0].value, params[1].value)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlogLikely\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/site-packages/scipy/stats/_distn_infrastructure.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m     \u001b[0m__call__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfreeze\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/site-packages/scipy/stats/_distn_infrastructure.py\u001b[0m in \u001b[0;36mfreeze\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m         \"\"\"\n\u001b[0;32m--> 863\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrv_frozen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/site-packages/scipy/stats/_distn_infrastructure.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dist, *args, **kwds)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;31m# create a new instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_updated_ctor_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0mshapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/site-packages/scipy/stats/_distn_infrastructure.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, momtype, a, b, xtol, badvalue, name, longname, shapes, extradoc, seed)\u001b[0m\n\u001b[1;32m   1770\u001b[0m                                             discrete='continuous')\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1772\u001b[0;31m                 \u001b[0mdct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistcont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1773\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sigma = Theta(0,300)\n",
    "mean = Theta(5000,5600)\n",
    "sigma.update(150)\n",
    "mean.update(5250)\n",
    "theta = np.asarray([mean,sigma])\n",
    "momentumGradientDecent(theta, 20, logLikely, df[\"mass\"], 0.001, normalPDF, 20, 0.0001,len(df[\"mass\"]), 0.9)\n",
    "print(theta[0].value, theta[1].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-40.8690503827529 0\n",
      "-35.69186508866551 1\n",
      "8.866867442520743 0\n",
      "24.706037793293945 1\n",
      "15.458556859812234 0\n",
      "21.52079603729362 1\n",
      "24.106013115670066 0\n",
      "0.7641511874680873 1\n",
      "-3.4963798707394744 0\n",
      "-10.378454745477939 1\n",
      "-2.198883759774617 0\n",
      "5.096208577924699 1\n",
      "-3.363398059264 0\n",
      "-15.362295342129073 1\n",
      "-2.356485606469505 0\n",
      "15.484150734664581 1\n",
      "-6.292075790952367 0\n",
      "-13.069616387838323 1\n",
      "2.229907049695612 0\n",
      "11.911145294106973 1\n",
      "2.578469429863617 0\n",
      "-14.977831713622436 1\n",
      "-1.0225021187579841 0\n",
      "11.877937447934528 1\n",
      "4.911954731142032 0\n",
      "-0.47760728557477705 1\n",
      "-3.0217355815693736 0\n",
      "-3.9342493500953424 1\n",
      "0.42516251596680377 0\n",
      "8.245900106885529 1\n",
      "1.296631774494017 0\n",
      "-16.916123699957097 1\n",
      "-4.751246871819603 0\n",
      "14.642694370195386 1\n",
      "0.690505084094184 0\n",
      "-2.6779035661093076 1\n",
      "0.7863665969125577 0\n",
      "3.7970262328599347 1\n",
      "2.5760523640201427 0\n",
      "-17.153153557046608 1\n",
      "max steps\n",
      "5299.735931174022 105.20962489819162\n"
     ]
    }
   ],
   "source": [
    "sigma = Theta(0,300)\n",
    "mean = Theta(5000,5600)\n",
    "sigma.update(90)\n",
    "mean.update(5250)\n",
    "theta = np.asarray([mean,sigma])\n",
    "nesterovGradientDecent(theta, 7, logLikely, df[\"mass\"], 0.001, normalPDF, 20, 0.00001,len(df[\"mass\"]), 0.9)\n",
    "print(theta[0].value, theta[1].value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the results\n",
    "\n",
    "1. Plot the PDF with best-fit values overlaid on to a histogram of the data.\n",
    "  - _**Hint**: to achieve the same normalisation for the data and the PDF, you may choose to use `density=True` in the arguments to `pyplot.hist`_\n",
    "1. Plot a 'likelihood trace' (i.e. the 'history' of the likelihood at each iteration) for the following cases:\n",
    "  - Several choices of fixed step size for batch gradient descent\n",
    "  - The effect of different methods of gradient-descent (6 in total)\n",
    "    - batch\n",
    "    - mini-batch\n",
    "    - stochastic\n",
    "    - all of the above using the Nesterov technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
